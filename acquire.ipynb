{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d125f9",
   "metadata": {},
   "source": [
    "# Acquire Data through Web Scraping Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96cdd5",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2064ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import strftime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887e688",
   "metadata": {},
   "source": [
    "### Codeup Blog Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ac2f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codeup.com/blog/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5a1905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get all links from the main blog page\n",
    "\n",
    "def get_blog_links():\n",
    "    \n",
    "    '''\n",
    "    This function returns a list of all the urls linked on Codeup's main blog page.\n",
    "    '''\n",
    "    \n",
    "    # imports\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    response = requests.get(\"https://codeup.com/blog/\", headers={\"user-agent\": \"Codeup DS Hopper\"})\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    links = [link.attrs[\"href\"] for link in soup.select(\".more-link\")]\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb6664ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function to get list of links\n",
    "# get_blog_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5419b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_codeup_blog_article(url):\n",
    "    \n",
    "    '''\n",
    "    This function: \n",
    "    takes url of blog post,\n",
    "    extracts info\n",
    "    returns the specified info as a dictionary\n",
    "    '''\n",
    "    \n",
    "    # imports\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    response = requests.get(url, headers={\"user-agent\": \"Codeup DS Hopper\"})\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    return {\n",
    "        \"title\": soup.select_one(\".entry-title\").text,\n",
    "        \"published\": soup.select_one(\".published\").text,\n",
    "        \"content\": soup.select_one(\".entry-content\").text.strip(),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc0e3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/dallas-newsletter/codeup-dallas-open-house/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "159481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_codeup_blog_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c31d8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return blog article info as a df\n",
    "def get_blog_articles_info():\n",
    "    \n",
    "    '''\n",
    "    This function: \n",
    "    uses get_blog_links to get blog post links from the main Codeup blog page,\n",
    "    extracts info using parse_codeup_blog_article,\n",
    "    returns the specified info as a df where each row contains a seperate blog title\n",
    "    '''\n",
    "    \n",
    "    # imports\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    links = get_blog_links()\n",
    "    df = pd.DataFrame([parse_codeup_blog_article(link) for link in links])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "251ed09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_blog_articles_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94706fe",
   "metadata": {},
   "source": [
    "To save this info so that if the website is updated, it won't change my data during exploration etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "327edf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "# today = strftime('%Y-%m-%d')\n",
    "# get_blog_articles().to_json(f'codeup_blog_{today}.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ece3f",
   "metadata": {},
   "source": [
    "### inshorts Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd4ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_news_card(card):\n",
    "    'Given a news card object, returns a dictionary of the relevant information.'\n",
    "    card_title = card.select_one('.news-card-title')\n",
    "    output = {}\n",
    "    output['title'] = card_title.find('a').text.strip()\n",
    "    output['published'] = card_title.select_one('.time').attrs['content']\n",
    "    output['author'] = card_title.select_one('.author').text\n",
    "    output['content'] = card.select_one('.news-card-content').div.text.strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_inshorts_page(url):\n",
    "    '''Given a url, returns a dataframe where each row is a news article from the url.\n",
    "    Infers the category from the last section of the url.'''\n",
    "    category = url.split('/')[-1]\n",
    "    response = requests.get(url, headers={'user-agent': 'Codeup DS'})\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    cards = soup.select('.news-card')\n",
    "    df = pd.DataFrame([parse_news_card(card) for card in cards])\n",
    "    df['category'] = category\n",
    "    return df\n",
    "\n",
    "def get_inshorts_articles():\n",
    "    '''\n",
    "    Returns a dataframe of news articles from the business, sports, technology, and entertainment sections of\n",
    "    inshorts.\n",
    "    '''\n",
    "    url = 'https://inshorts.com/en/read/{}'\n",
    "    categories = ['business', 'sports', 'technology', 'entertainment']\n",
    "    df = pd.concat([parse_inshorts_page(url.format(cat)) for cat in categories])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c27c0",
   "metadata": {},
   "source": [
    "To save this info so that if the website is updated, it won't change my data during exploration etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f75a1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = strftime('%Y-%m-%d')\n",
    "# get_inshorts_articles().to_json(f'inshorts-{today}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12886465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('inshorts-2022-02-07.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b251232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
